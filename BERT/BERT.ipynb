{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Named Entity Recognition\n",
    "\n",
    "## Authors:\n",
    "- Giovanni Nocerino\n",
    "- Melita Freiberga\n",
    "- Niccolo' Pagano\n",
    "\n",
    "## Requirements:\n",
    "- transformers\n",
    "- torch\n",
    "\n",
    "## The notebook will:\n",
    "1. Load and preprocess the dataset\n",
    "2. Perform Named Entity Recognition (NER) using different BERT models\n",
    "3. Visualize the results of NER\n",
    "\n",
    "## Returns:\n",
    "None. Displays the named entities recognized in the provided text examples using various BERT models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example sentences:\n",
    "\n",
    "\"Tesla was a Serbian-American inventor best known for his contributions to alternating current technology.\" \"Meanwhile, Tesla, founded by the famous enterpreneur Elon Musk, is a leading manufacturer of electric vehicles.\"\n",
    "\n",
    "\"The UEFA Champions League final between Real Madrid and Manchester City took place in Istanbul, Turkey, in 2024.\" \"Madrid is the capital of Spain and Manchester is an industrial city in the UK.\"\n",
    "\n",
    "\"The Amazon rainforest, spanning Brazil, Peru, and Colombia, is critical for global biodiversity, while the website Amazon, founded by Jeff Bezos is a popular online shopping service.\"\n",
    "\n",
    "\"Paris Hilton shared her travel vlog about Paris, including her favorite spots near the Eiffel Tower and Champs-Élysées.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BERT-base** fine-tuned specifically for NER on the CoNLL-2003 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "texts = [(\"Tesla was a Serbian-American inventor best known for his contributions to alternating current technology.\"\n",
    "        \"Meanwhile, Tesla, founded by the famous enterpreneur Elon Musk, is a leading manufacturer of electric vehicles.\"), (\"The UEFA Champions League final between Real Madrid and Manchester City took place in Istanbul, Turkey, in 2024.\"\n",
    "        \"Madrid is the capital of Spain and Manchester is an industrial city in the UK.\"), (\"The Amazon rainforest, spanning Brazil, Peru, and Colombia, is critical for global biodiversity, while the website Amazon, founded by Jeff Bezos is a popular online shopping service.\"), (\"Paris Hilton shared her travel vlog about Paris, including her favorite spots near the Eiffel Tower and Champs-Élysées.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# Create a pipeline for NER\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities for example 1:\n",
      " - Tesla (PER): 0.84\n",
      " - Serbian (MISC): 1.00\n",
      " - American (MISC): 0.71\n",
      " - Tesla (ORG): 0.99\n",
      " - Elon Musk (ORG): 0.99\n",
      "\n",
      "Named Entities for example 2:\n",
      " - UEFA Champions League (MISC): 1.00\n",
      " - Real Madrid (ORG): 1.00\n",
      " - Manchester City (ORG): 0.86\n",
      " - Istanbul (LOC): 1.00\n",
      " - Turkey (LOC): 1.00\n",
      " - Madrid (LOC): 1.00\n",
      " - Spain (LOC): 1.00\n",
      " - Manchester (LOC): 1.00\n",
      " - UK (LOC): 1.00\n",
      "\n",
      "Named Entities for example 3:\n",
      " - Amazon (LOC): 1.00\n",
      " - Brazil (LOC): 1.00\n",
      " - Peru (LOC): 1.00\n",
      " - Colombia (LOC): 1.00\n",
      " - Amazon (ORG): 0.98\n",
      " - Jeff Bezos (PER): 0.86\n",
      "\n",
      "Named Entities for example 4:\n",
      " - Paris Hilton (PER): 0.98\n",
      " - Paris (LOC): 1.00\n",
      " - E (LOC): 0.99\n",
      " - ##iff (LOC): 0.77\n",
      " - ##el Tower (LOC): 0.95\n",
      " - Champs - Élysées (LOC): 0.91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(texts):\n",
    "    # Perform NER\n",
    "    results = ner_pipeline(text)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Named Entities for example {i+1}:\")\n",
    "    for entity in results:\n",
    "        print(f\" - {entity['word']} ({entity['entity_group']}): {entity['score']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BERT-large-case** fine-tuned specifically for NER on the CoNLL-2003 dataset\n",
    "\n",
    "\n",
    "* **Larger version of BERT**, with:\n",
    "  \n",
    "    *\t24 transformer layers (vs. 12 in BERT-base).\n",
    "  \n",
    "    *\t1024 hidden dimensions (vs. 768 in BERT-base).\n",
    "  \n",
    "    *\t16 attention heads per layer (vs. 12 in BERT-base).\n",
    "\n",
    "* **Preserves capitalization** during tokenization and training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "texts = [(\"Tesla was a Serbian-American inventor best known for his contributions to alternating current technology.\"\n",
    "        \"Meanwhile, Tesla, founded by the famous enterpreneur Elon Musk, is a leading manufacturer of electric vehicles.\"), (\"The UEFA Champions League final between Real Madrid and Manchester City took place in Istanbul, Turkey, in 2024.\"\n",
    "        \"Madrid is the capital of Spain and Manchester is an industrial city in the UK.\"), (\"The Amazon rainforest, spanning Brazil, Peru, and Colombia, is critical for global biodiversity, while the website Amazon, founded by Jeff Bezos is a popular online shopping service.\"), (\"Paris Hilton shared her travel vlog about Paris, including her favorite spots near the Eiffel Tower and Champs-Élysées.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7ebc5bc1a3498a8b16b508ef1b66b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59db790c41924943adc2a26d0a89eff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe181158e8f46cfaed5cfd7c43e4599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d120f72cbbf40ea82cb4b21a2a25eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# Create a pipeline for NER\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities for example 1:\n",
      " - Tesla (PER): 1.00\n",
      " - Serbian - American (MISC): 0.88\n",
      " - Tesla (ORG): 0.99\n",
      " - Elon Musk (PER): 1.00\n",
      "\n",
      "Named Entities for example 2:\n",
      " - UEFA Champions League (MISC): 0.99\n",
      " - Real Madrid (ORG): 1.00\n",
      " - Manchester City (ORG): 1.00\n",
      " - Istanbul (LOC): 1.00\n",
      " - Turkey (LOC): 1.00\n",
      " - Madrid (LOC): 1.00\n",
      " - Spain (LOC): 1.00\n",
      " - Manchester (LOC): 1.00\n",
      " - UK (LOC): 1.00\n",
      "\n",
      "Named Entities for example 3:\n",
      " - Amazon (LOC): 1.00\n",
      " - Brazil (LOC): 1.00\n",
      " - Peru (LOC): 1.00\n",
      " - Colombia (LOC): 1.00\n",
      " - Amazon (ORG): 0.95\n",
      " - Jeff Bezos (PER): 0.95\n",
      "\n",
      "Named Entities for example 4:\n",
      " - Paris Hilton (PER): 1.00\n",
      " - Paris (LOC): 1.00\n",
      " - Eiffel Tower (LOC): 0.92\n",
      " - Champs - Ély (LOC): 0.88\n",
      " - ##s (MISC): 0.45\n",
      " - ##ées (LOC): 0.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(texts):\n",
    "    # Perform NER\n",
    "    results = ner_pipeline(text)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Named Entities for example {i+1}:\")\n",
    "    for entity in results:\n",
    "        print(f\" - {entity['word']} ({entity['entity_group']}): {entity['score']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With combined subwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities for example 1:\n",
      " - Tesla (PER): 1.00\n",
      " - Serbian - American (MISC): 0.88\n",
      " - Tesla (ORG): 0.99\n",
      " - Elon Musk (PER): 1.00\n",
      "\n",
      "Named Entities for example 2:\n",
      " - UEFA Champions League (MISC): 0.99\n",
      " - Real Madrid (ORG): 1.00\n",
      " - Manchester City (ORG): 1.00\n",
      " - Istanbul (LOC): 1.00\n",
      " - Turkey (LOC): 1.00\n",
      " - Madrid (LOC): 1.00\n",
      " - Spain (LOC): 1.00\n",
      " - Manchester (LOC): 1.00\n",
      " - UK (LOC): 1.00\n",
      "\n",
      "Named Entities for example 3:\n",
      " - Amazon (LOC): 1.00\n",
      " - Brazil (LOC): 1.00\n",
      " - Peru (LOC): 1.00\n",
      " - Colombia (LOC): 1.00\n",
      " - Amazon (ORG): 0.95\n",
      " - Jeff Bezos (PER): 0.95\n",
      "\n",
      "Named Entities for example 4:\n",
      " - Paris Hilton (PER): 1.00\n",
      " - Paris (LOC): 1.00\n",
      " - Eiffel Tower (LOC): 0.92\n",
      " - Champs - Élysées (LOC): 0.77\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def combine_subwords(results):\n",
    "    combined_results = []\n",
    "    temp_word = \"\"\n",
    "    temp_group = None\n",
    "    temp_score = []\n",
    "\n",
    "    for entity in results:\n",
    "        word = entity['word']\n",
    "        group = entity['entity_group']\n",
    "        score = entity['score']\n",
    "\n",
    "        # Handle subwords\n",
    "        if word.startswith(\"##\"):\n",
    "            temp_word += word[2:]\n",
    "            temp_score.append(score)\n",
    "        else:\n",
    "            if temp_word:\n",
    "                # Save the previous combined word\n",
    "                combined_results.append({\n",
    "                    \"word\": temp_word,\n",
    "                    \"entity_group\": temp_group,\n",
    "                    \"score\": sum(temp_score) / len(temp_score)  # Average score\n",
    "                })\n",
    "            temp_word = word\n",
    "            temp_group = group\n",
    "            temp_score = [score]\n",
    "\n",
    "    # Append the last word\n",
    "    if temp_word:\n",
    "        combined_results.append({\n",
    "            \"word\": temp_word,\n",
    "            \"entity_group\": temp_group,\n",
    "            \"score\": sum(temp_score) / len(temp_score)\n",
    "        })\n",
    "\n",
    "    return combined_results\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    # Perform NER\n",
    "    results = ner_pipeline(text)\n",
    "\n",
    "    # Combine subwords\n",
    "    combined_results = combine_subwords(results)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Named Entities for example {i+1}:\")\n",
    "    for entity in combined_results:\n",
    "        print(f\" - {entity['word']} ({entity['entity_group']}): {entity['score']:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual model XLM-Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "texts = [(\"Tesla was a Serbian-American inventor best known for his contributions to alternating current technology.\"\n",
    "        \"Meanwhile, Tesla, founded by the famous enterpreneur Elon Musk, is a leading manufacturer of electric vehicles.\"), (\"The UEFA Champions League final between Real Madrid and Manchester City took place in Istanbul, Turkey, in 2024.\"\n",
    "        \"Madrid is the capital of Spain and Manchester is an industrial city in the UK.\"), (\"The Amazon rainforest, spanning Brazil, Peru, and Colombia, is critical for global biodiversity, while the website Amazon, founded by Jeff Bezos is a popular online shopping service.\"), (\"Paris Hilton shared her travel vlog about Paris, including her favorite spots near the Eiffel Tower and Champs-Élysées.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e137d792ce424325a4293217f96b5794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f73a065995f415faa8d92fdc2151a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/982 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7988e73226444b509ee0f1c1e6d8479e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c158a447a34af19bc2be2be0ce54cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vannileonardo/anaconda3/envs/Prosthetic_Arm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6750588882e4559ba6d3a54c73be7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"Davlan/xlm-roberta-large-ner-hrl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# Create a pipeline for NER\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities for example 1:\n",
      " - Tesla (ORG): 1.00\n",
      " - Tesla (ORG): 1.00\n",
      " - Elon Musk (PER): 1.00\n",
      "\n",
      "Named Entities for example 2:\n",
      " - Real Madrid (ORG): 1.00\n",
      " - Manchester City (ORG): 1.00\n",
      " - Istanbul (LOC): 1.00\n",
      " - Turkey (LOC): 1.00\n",
      " - Madrid (LOC): 0.99\n",
      " - Spain (LOC): 1.00\n",
      " - Manchester (LOC): 1.00\n",
      " - UK (LOC): 1.00\n",
      "\n",
      "Named Entities for example 3:\n",
      " - Amazon (LOC): 1.00\n",
      " - Brazil (LOC): 1.00\n",
      " - Peru (LOC): 1.00\n",
      " - Colombia (LOC): 1.00\n",
      " - Amazon (ORG): 1.00\n",
      " - Jeff Bezos (PER): 1.00\n",
      "\n",
      "Named Entities for example 4:\n",
      " - Paris Hilton (PER): 1.00\n",
      " - Paris (LOC): 1.00\n",
      " - Eiffel Tower (LOC): 1.00\n",
      " - Champs-Élysées (LOC): 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(texts):\n",
    "    # Perform NER\n",
    "    results = ner_pipeline(text)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Named Entities for example {i+1}:\")\n",
    "    for entity in results:\n",
    "        print(f\" - {entity['word']} ({entity['entity_group']}): {entity['score']:.2f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Prosthetic_Arm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
